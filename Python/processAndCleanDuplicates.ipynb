{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities have been updated and duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alias_mapping = {\n",
    "    \"UP\": \"Uttar Pradesh\",\n",
    "    \"U.P.\": \"Uttar Pradesh\",\n",
    "    \"Uttar Pradesh\": \"Uttar Pradesh\",\n",
    "    \"Uttar Pradesh 's\" : \"Uttar Pradesh\" # Ensuring canonical name maps to itself\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Load entities.csv\n",
    "entities_df = pd.read_csv('cleaned_entities2024.csv')\n",
    "\n",
    "# Replace aliases with canonical names\n",
    "entities_df['entity'] = entities_df['entity'].apply(lambda x: alias_mapping.get(x, x))\n",
    "\n",
    "# Remove duplicate entities\n",
    "entities_df = entities_df.drop_duplicates(subset=['entity', 'type']).reset_index(drop=True)\n",
    "\n",
    "# Save the updated entities.csv\n",
    "entities_df.to_csv('/home/vidur/mediagraph/Python/final files/cleaned_entities2024.csv', index=False)\n",
    "\n",
    "print(\"Entities have been updated and duplicates removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "                entity1                   entity2  weight  \\\n",
      "0               Haryana  Jammu & Kashmir Alliance       8   \n",
      "1                 Noida                     YEIDA       1   \n",
      "2  Ekta Dakaunda-Dhaner             Swaiman Singh       1   \n",
      "3               Madhura             Swaiman Singh       1   \n",
      "4     M S Swaminathan's             Swaiman Singh       1   \n",
      "\n",
      "                                dates  \n",
      "0  02-2024, 03-2024, 04-2024, 05-2024  \n",
      "1                             01-2024  \n",
      "2                             02-2024  \n",
      "3                             02-2024  \n",
      "4                             02-2024  \n",
      "\n",
      "Missing 'entity1' entries: 0\n",
      "Missing 'entity2' entries: 0\n",
      "Missing 'dates' entries: 0\n",
      "\n",
      "Aggregated Data:\n",
      "   weight dates entity1         entity2\n",
      "0       2    []    \"BJP    Aditya Yadav\n",
      "1       2    []    \"BJP  Akhilesh Yadav\n",
      "2       2    []    \"BJP             BJP\n",
      "3       2    []    \"BJP        Bareilly\n",
      "4       2    []    \"BJP          Budaun\n",
      "\n",
      "Entities have been updated and duplicates removed.\n",
      "Co-occurrences have been updated and aggregated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Define the alias to canonical mapping\n",
    "alias_mapping = {\n",
    "    \"UP\": \"Uttar Pradesh\",\n",
    "    \"U.P.\": \"Uttar Pradesh\",\n",
    "    \"Uttar Pradesh\": \"Uttar Pradesh\",  # Ensuring canonical name maps to itself\n",
    "    # Add more mappings as needed\n",
    "    # \"Delhi\": \"NCT of Delhi\",\n",
    "    # \"NCT Delhi\": \"NCT of Delhi\",\n",
    "}\n",
    "\n",
    "# Paths to the CSV files\n",
    "entities_path = '/home/vidur/mediagraph/Python/cleaned_entities2024.csv'\n",
    "cooccurrences_path = '/home/vidur/mediagraph/Python/cleaned_co_occurrence2024.csv'\n",
    "updated_entities_path = '/home/vidur/mediagraph/Python/cleaned_entities2024_2.csv'\n",
    "updated_cooccurrences_path = '/home/vidur/mediagraph/Python/cleaned_co_occurrence2024_2.csv'\n",
    "\n",
    "# Step 1: Load and Clean cooccurrences.csv\n",
    "cooccurrences_df = pd.read_csv(cooccurrences_path)\n",
    "\n",
    "print(\"Initial Data:\")\n",
    "print(cooccurrences_df.head())\n",
    "\n",
    "# Check for missing values in 'entity1', 'entity2', and 'dates'\n",
    "missing_entity1 = cooccurrences_df['entity1'].isnull().sum()\n",
    "missing_entity2 = cooccurrences_df['entity2'].isnull().sum()\n",
    "missing_dates = cooccurrences_df['dates'].isnull().sum()\n",
    "\n",
    "print(f\"\\nMissing 'entity1' entries: {missing_entity1}\")\n",
    "print(f\"Missing 'entity2' entries: {missing_entity2}\")\n",
    "print(f\"Missing 'dates' entries: {missing_dates}\")\n",
    "\n",
    "# Drop rows with missing 'entity1' or 'entity2'\n",
    "cooccurrences_df = cooccurrences_df.dropna(subset=['entity1', 'entity2'])\n",
    "\n",
    "# Handle missing 'dates' by replacing them with empty lists\n",
    "cooccurrences_df['dates'] = cooccurrences_df['dates'].fillna('[]')\n",
    "\n",
    "# Convert 'entity1' and 'entity2' to strings\n",
    "cooccurrences_df['entity1'] = cooccurrences_df['entity1'].astype(str)\n",
    "cooccurrences_df['entity2'] = cooccurrences_df['entity2'].astype(str)\n",
    "\n",
    "# Replace aliases with canonical names\n",
    "cooccurrences_df['entity1'] = cooccurrences_df['entity1'].apply(lambda x: alias_mapping.get(x, x))\n",
    "cooccurrences_df['entity2'] = cooccurrences_df['entity2'].apply(lambda x: alias_mapping.get(x, x))\n",
    "\n",
    "# Function to create a sorted tuple key for each pair to handle unordered pairs\n",
    "def create_sorted_pair(row):\n",
    "    return tuple(sorted([row['entity1'], row['entity2']]))\n",
    "\n",
    "# Apply the function to create a new column with sorted pairs\n",
    "cooccurrences_df['pair'] = cooccurrences_df.apply(create_sorted_pair, axis=1)\n",
    "\n",
    "# Function to safely evaluate 'dates' strings to lists\n",
    "def safe_eval_dates(dates_str):\n",
    "    try:\n",
    "        dates = ast.literal_eval(dates_str)\n",
    "        if isinstance(dates, list):\n",
    "            return dates\n",
    "        else:\n",
    "            return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Apply the function to ensure 'dates' are lists\n",
    "cooccurrences_df['dates'] = cooccurrences_df['dates'].apply(safe_eval_dates)\n",
    "\n",
    "# Group by the sorted pair and aggregate weights and dates\n",
    "aggregated_df = cooccurrences_df.groupby('pair').agg({\n",
    "    'weight': 'sum',\n",
    "    'dates': lambda dates: sorted(set([date for sublist in dates for date in sublist]))\n",
    "}).reset_index()\n",
    "\n",
    "# Split the pair back into 'entity1' and 'entity2'\n",
    "aggregated_df[['entity1', 'entity2']] = pd.DataFrame(aggregated_df['pair'].tolist(), index=aggregated_df.index)\n",
    "\n",
    "# Drop the 'pair' column as it's no longer needed\n",
    "aggregated_df = aggregated_df.drop(columns=['pair'])\n",
    "\n",
    "# Convert the dates list to string format suitable for Neo4j\n",
    "aggregated_df['dates'] = aggregated_df['dates'].apply(lambda x: str(x))\n",
    "\n",
    "# Display the aggregated DataFrame\n",
    "print(\"\\nAggregated Data:\")\n",
    "print(aggregated_df.head())\n",
    "\n",
    "# Step 2: Load and Update entities.csv\n",
    "entities_df = pd.read_csv(entities_path)\n",
    "\n",
    "# Replace aliases with canonical names\n",
    "entities_df['entity'] = entities_df['entity'].apply(lambda x: alias_mapping.get(x, x))\n",
    "\n",
    "# Remove duplicate entities\n",
    "entities_df = entities_df.drop_duplicates(subset=['entity', 'type']).reset_index(drop=True)\n",
    "\n",
    "# Save the updated entities.csv\n",
    "entities_df.to_csv(updated_entities_path, index=False)\n",
    "print(\"\\nEntities have been updated and duplicates removed.\")\n",
    "\n",
    "# Step 3: Save the updated cooccurrences.csv\n",
    "aggregated_df.to_csv(updated_cooccurrences_path, index=False)\n",
    "print(\"Co-occurrences have been updated and aggregated successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
