{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "from metaphone import doublemetaphone\n",
    "from difflib import SequenceMatcher\n",
    "from ast import literal_eval\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('/home/vidur/mediagraph/data/output.csv')\n",
    "\n",
    "# Fill missing values in 'Text' column\n",
    "df['Text'] = df['Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'Title': ['Punjab minister orders probe into jail assault on farm activist'],\n",
    "    'Date': ['01-01-2020'],\n",
    "    'Text': ['Your full text here'],\n",
    "    'Entities': [\"[('BATHINDA', 'GPE'), ('Punjab', 'GPE'), ('Sukhjinder Singh Randhawa', 'PERSON')]\"],\n",
    "    'Resolved Entities': [\"[('BATHINDA', 'GPE'), ('Punjab', 'GPE'), ('Sukhjinder Singh Randhawa', 'PERSON')]\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'Resolved Entities' column to extract entities\n",
    "def parse_resolved_entities(resolved_entities_str):\n",
    "    try:\n",
    "        # Convert the string representation of the list to an actual list\n",
    "        entities = literal_eval(resolved_entities_str)\n",
    "        return entities\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the parsing function\n",
    "df['Parsed_Entities'] = df['Resolved Entities'].apply(parse_resolved_entities)\n",
    "\n",
    "# Extract person-type entities\n",
    "def extract_person_entities(entities):\n",
    "    return [entity[0] for entity in entities if entity[1] in ['PERSON', 'POL', 'DIR']]\n",
    "\n",
    "# Apply extraction to get person entities\n",
    "df['Person_Entities'] = df['Parsed_Entities'].apply(extract_person_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all person entities from all articles\n",
    "all_entities = df['Person_Entities'].explode().dropna().unique().tolist()\n",
    "\n",
    "# Preprocess entity names\n",
    "def preprocess_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z\\s\\']', '', name)\n",
    "    name = name.lower().strip()\n",
    "    return name\n",
    "\n",
    "all_entities = [preprocess_name(name) for name in all_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Matching Functions\n",
    "def levenshtein_ratio(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def metaphone_match(name1, name2):\n",
    "    meta1 = doublemetaphone(name1)\n",
    "    meta2 = doublemetaphone(name2)\n",
    "    return any(m1 == m2 for m1 in meta1 for m2 in meta2 if m1 and m2)\n",
    "\n",
    "def check_match(name1, name2):\n",
    "    # Remove periods, convert to lowercase, and split names\n",
    "    name1_words = name1.replace('.', '').lower().split()\n",
    "    name2_words = name2.replace('.', '').lower().split()\n",
    "    \n",
    "    # Check if either name is a single word (ignore such cases)\n",
    "    if len(name1_words) < 2 or len(name2_words) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Extract last names\n",
    "    last_name1 = name1_words[-1]\n",
    "    last_name2 = name2_words[-1]\n",
    "    \n",
    "    # Check if last names match or if one is the initial of the other\n",
    "    last_names_match = (last_name1 == last_name2 or \n",
    "                        (len(last_name1) == 1 and last_name1 == last_name2[0]) or \n",
    "                        (len(last_name2) == 1 and last_name2 == last_name1[0]))\n",
    "    \n",
    "    # Extract first names or initials\n",
    "    first_name1 = name1_words[0]\n",
    "    first_name2 = name2_words[0]\n",
    "    \n",
    "    # Check if first names match or initials correspond\n",
    "    first_names_match = (first_name1[0] == first_name2[0])\n",
    "    \n",
    "    # Use a lower threshold for Levenshtein ratio when comparing full names\n",
    "    if len(first_name1) > 1 and len(first_name2) > 1:\n",
    "        lev_ratio = levenshtein_ratio(first_name1, first_name2)\n",
    "    else:\n",
    "        lev_ratio = 1  # Assume perfect match if one is just an initial\n",
    "    \n",
    "    # Check Metaphone phonetic similarity for both first and last names combined\n",
    "    meta_match = metaphone_match(' '.join(name1_words), ' '.join(name2_words))\n",
    "    \n",
    "    # Determine final match based on OR condition\n",
    "    if (last_names_match and (first_names_match or lev_ratio >= 0.8)) or meta_match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clusters based on rule-based matching\n",
    "clusters = []\n",
    "unclustered_entities = set(all_entities)\n",
    "\n",
    "while unclustered_entities:\n",
    "    base_entity = unclustered_entities.pop()\n",
    "    cluster = [base_entity]\n",
    "    to_check = set(unclustered_entities)\n",
    "    for other_entity in to_check:\n",
    "        if check_match(base_entity, other_entity):\n",
    "            cluster.append(other_entity)\n",
    "            unclustered_entities.remove(other_entity)\n",
    "    clusters.append(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus for Word2Vec (using the text data)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['Tokens'] = df['Text'].apply(preprocess_text)\n",
    "corpus = df['Tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "embedding_size = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "model = Word2Vec(sentences=corpus,\n",
    "                 vector_size=embedding_size,\n",
    "                 window=window_size,\n",
    "                 min_count=min_count,\n",
    "                 workers=workers,\n",
    "                 sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for entities\n",
    "def get_name_embedding(name, model):\n",
    "    words = name.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "        else:\n",
    "            word_vectors.append(np.zeros(model.vector_size))\n",
    "    if word_vectors:\n",
    "        name_embedding = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        name_embedding = np.zeros(model.vector_size)\n",
    "    return name_embedding\n",
    "\n",
    "name_embeddings = [get_name_embedding(name, model) for name in all_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering on embeddings\n",
    "clustering_model = DBSCAN(eps=0.5, min_samples=1, metric='cosine')\n",
    "cluster_labels = clustering_model.fit_predict(name_embeddings)\n",
    "\n",
    "# Group entities by clusters\n",
    "vector_clusters = {}\n",
    "for label, name in zip(cluster_labels, all_entities):\n",
    "    vector_clusters.setdefault(label, []).append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clusters from both methods\n",
    "# Map entities to their rule-based cluster IDs\n",
    "entity_to_rule_cluster = {}\n",
    "for cluster_id, cluster in enumerate(clusters):\n",
    "    for entity in cluster:\n",
    "        entity_to_rule_cluster[entity] = cluster_id\n",
    "\n",
    "# Merge clusters based on overlapping entities\n",
    "combined_clusters = {}\n",
    "for vector_cluster_id, vector_cluster in vector_clusters.items():\n",
    "    combined_cluster = set()\n",
    "    for entity in vector_cluster:\n",
    "        rule_cluster_id = entity_to_rule_cluster.get(entity)\n",
    "        if rule_cluster_id is not None:\n",
    "            combined_cluster.update(clusters[rule_cluster_id])\n",
    "        else:\n",
    "            combined_cluster.add(entity)\n",
    "    combined_clusters[vector_cluster_id] = list(combined_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1682440/3337648790.py:2: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  es = Elasticsearch(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    \"https://your_elasticsearch_endpoint\",\n",
    "    http_auth=('username', 'password')  # Use appropriate authentication\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update Elasticsearch index\n",
    "def update_elasticsearch(name_list, cluster_id):\n",
    "    # Decide on a representative name for the entity (e.g., the most frequent name)\n",
    "    representative_name = max(name_list, key=len)\n",
    "    \n",
    "    # Prepare aliases (other names in the cluster)\n",
    "    aliases = [name for name in name_list if name != representative_name]\n",
    "    \n",
    "    # Check if an entity with the representative name already exists\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"name\": representative_name\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"resolved_entities\", body=search_body)\n",
    "    if response['hits']['hits']:\n",
    "        # Entity exists, update aliases\n",
    "        entity_id = response['hits']['hits'][0]['_id']\n",
    "        existing_aliases = response['hits']['hits'][0]['_source'].get('aliases', '')\n",
    "        existing_aliases = existing_aliases.split(';') if existing_aliases else []\n",
    "        all_aliases = set(existing_aliases + aliases)\n",
    "        es.update(\n",
    "            index=\"resolved_entities\",\n",
    "            id=entity_id,\n",
    "            body={\"doc\": {\"aliases\": ';'.join(all_aliases)}}\n",
    "        )\n",
    "    else:\n",
    "        # Entity does not exist, create a new one\n",
    "        es.index(index=\"resolved_entities\", body={\n",
    "            \"name\": representative_name,\n",
    "            \"aliases\": ';'.join(aliases),\n",
    "            \"type\": \"PERSON\"  # Adjust the type as needed\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Connection error caused by: ConnectionError(Connection error caused by: NameResolutionError(<elastic_transport._node._urllib3_chain_certs.HTTPSConnection object at 0x7faeeedd7130>: Failed to resolve 'your_elasticsearch_endpoint' ([Errno -3] Temporary failure in name resolution)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:967\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    966\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    968\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py:167\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[0;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[1;32m    165\u001b[0m     body_to_send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_to_send\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m HttpHeaders(response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:449\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m error:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elastic_transport/_node/_urllib3_chain_certs.py:96\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03mCalled right before a request is made, after the socket is created.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPSConnectionPool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elastic_assert_fingerprint:\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elastic_transport/_node/_urllib3_chain_certs.py:45\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Hack to prevent a warning within HTTPSConnectionPool._validate_conn()\u001b[39;00m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/urllib3/connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <elastic_transport._node._urllib3_chain_certs.HTTPSConnection object at 0x7faeeedd4610>: Failed to resolve 'your_elasticsearch_endpoint' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Update Elasticsearch with combined clusters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster_id, name_list \u001b[38;5;129;01min\u001b[39;00m combined_clusters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mupdate_elasticsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mupdate_elasticsearch\u001b[0;34m(name_list, cluster_id)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if an entity with the representative name already exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m search_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresolved_entities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Entity exists, update aliases\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     entity_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:446\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:4149\u001b[0m, in \u001b[0;36mElasticsearch.search\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, force_synthetic_source, from_, highlight, human, ignore_throttled, ignore_unavailable, include_named_queries_score, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, retriever, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version, body)\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4148\u001b[0m     __headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   4150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__path_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:316\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     target \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 316\u001b[0m meta, resp_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_on_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m299\u001b[39m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m ):\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elastic_transport/_transport.py:342\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta, otel_span)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     otel_span\u001b[38;5;241m.\u001b[39mset_node_metadata(node\u001b[38;5;241m.\u001b[39mhost, node\u001b[38;5;241m.\u001b[39mport, node\u001b[38;5;241m.\u001b[39mbase_url, target)\n\u001b[0;32m--> 342\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m [status:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m duration:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m         )\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py:202\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[0;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[1;32m    194\u001b[0m         err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e), errors\u001b[38;5;241m=\u001b[39m(e,))\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(\n\u001b[1;32m    196\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    197\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m         exception\u001b[38;5;241m=\u001b[39merr,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m meta \u001b[38;5;241m=\u001b[39m ApiResponseMeta(\n\u001b[1;32m    205\u001b[0m     node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    206\u001b[0m     duration\u001b[38;5;241m=\u001b[39mduration,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresponse_headers,\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(\n\u001b[1;32m    212\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    213\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m     response\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    218\u001b[0m )\n",
      "\u001b[0;31mConnectionError\u001b[0m: Connection error caused by: ConnectionError(Connection error caused by: NameResolutionError(<elastic_transport._node._urllib3_chain_certs.HTTPSConnection object at 0x7faeeedd7130>: Failed to resolve 'your_elasticsearch_endpoint' ([Errno -3] Temporary failure in name resolution)))"
     ]
    }
   ],
   "source": [
    "# Update Elasticsearch with combined clusters\n",
    "for cluster_id, name_list in combined_clusters.items():\n",
    "    update_elasticsearch(name_list, cluster_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## FINAL\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import re\n",
    "from metaphone import doublemetaphone\n",
    "from difflib import SequenceMatcher\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'Title': ['Punjab minister orders probe into jail assault on farm activist'],\n",
    "    'Date': ['01-01-2020'],\n",
    "    'Text': ['Your full text here'],\n",
    "    'Entities': [\"[('BATHINDA', 'GPE'), ('Punjab', 'GPE'), ('Sukhjinder Singh Randhawa', 'PERSON')]\"],\n",
    "    'Resolved Entities': [\"[('BATHINDA', 'GPE'), ('Punjab', 'GPE'), ('Sukhjinder Singh Randhawa', 'PERSON')]\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'Resolved Entities' column to extract entities\n",
    "def parse_resolved_entities(resolved_entities_str):\n",
    "    try:\n",
    "        # Convert the string representation of the list to an actual list\n",
    "        entities = literal_eval(resolved_entities_str)\n",
    "        return entities\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the parsing function\n",
    "df['Parsed_Entities'] = df['Resolved Entities'].apply(parse_resolved_entities)\n",
    "\n",
    "# Extract person-type entities\n",
    "def extract_person_entities(entities):\n",
    "    return [entity[0] for entity in entities if entity[1] in ['PERSON', 'POL', 'DIR']]\n",
    "\n",
    "# Apply extraction to get person entities\n",
    "df['Person_Entities'] = df['Parsed_Entities'].apply(extract_person_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all person entities from all articles\n",
    "all_entities = df['Person_Entities'].explode().dropna().unique().tolist()\n",
    "\n",
    "# Preprocess entity names\n",
    "def preprocess_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z\\s\\']', '', name)\n",
    "    name = name.lower().strip()\n",
    "    return name\n",
    "\n",
    "all_entities = [preprocess_name(name) for name in all_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Matching Functions\n",
    "def levenshtein_ratio(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def metaphone_match(name1, name2):\n",
    "    meta1 = doublemetaphone(name1)\n",
    "    meta2 = doublemetaphone(name2)\n",
    "    return any(m1 == m2 for m1 in meta1 for m2 in meta2 if m1 and m2)\n",
    "\n",
    "def check_match(name1, name2):\n",
    "    # Remove periods, convert to lowercase, and split names\n",
    "    name1_words = name1.replace('.', '').lower().split()\n",
    "    name2_words = name2.replace('.', '').lower().split()\n",
    "    \n",
    "    # Check if either name is a single word (ignore such cases)\n",
    "    if len(name1_words) < 2 or len(name2_words) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Extract last names\n",
    "    last_name1 = name1_words[-1]\n",
    "    last_name2 = name2_words[-1]\n",
    "    \n",
    "    # Check if last names match or if one is the initial of the other\n",
    "    last_names_match = (last_name1 == last_name2 or \n",
    "                        (len(last_name1) == 1 and last_name1 == last_name2[0]) or \n",
    "                        (len(last_name2) == 1 and last_name2 == last_name1[0]))\n",
    "    \n",
    "    # Extract first names or initials\n",
    "    first_name1 = name1_words[0]\n",
    "    first_name2 = name2_words[0]\n",
    "    \n",
    "    # Check if first names match or initials correspond\n",
    "    first_names_match = (first_name1[0] == first_name2[0])\n",
    "    \n",
    "    # Use a lower threshold for Levenshtein ratio when comparing full names\n",
    "    if len(first_name1) > 1 and len(first_name2) > 1:\n",
    "        lev_ratio = levenshtein_ratio(first_name1, first_name2)\n",
    "    else:\n",
    "        lev_ratio = 1  # Assume perfect match if one is just an initial\n",
    "    \n",
    "    # Check Metaphone phonetic similarity for both first and last names combined\n",
    "    meta_match = metaphone_match(' '.join(name1_words), ' '.join(name2_words))\n",
    "    \n",
    "    # Determine final match based on OR condition\n",
    "    if (last_names_match and (first_names_match or lev_ratio >= 0.8)) or meta_match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clusters based on rule-based matching\n",
    "clusters = []\n",
    "unclustered_entities = set(all_entities)\n",
    "\n",
    "while unclustered_entities:\n",
    "    base_entity = unclustered_entities.pop()\n",
    "    cluster = [base_entity]\n",
    "    to_check = set(unclustered_entities)\n",
    "    for other_entity in to_check:\n",
    "        if check_match(base_entity, other_entity):\n",
    "            cluster.append(other_entity)\n",
    "            unclustered_entities.remove(other_entity)\n",
    "    clusters.append(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus for Word2Vec (using the text data)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['Tokens'] = df['Text'].apply(preprocess_text)\n",
    "corpus = df['Tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "embedding_size = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "model = Word2Vec(sentences=corpus,\n",
    "                 vector_size=embedding_size,\n",
    "                 window=window_size,\n",
    "                 min_count=min_count,\n",
    "                 workers=workers,\n",
    "                 sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for entities\n",
    "def get_name_embedding(name, model):\n",
    "    words = name.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "        else:\n",
    "            word_vectors.append(np.zeros(model.vector_size))\n",
    "    if word_vectors:\n",
    "        name_embedding = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        name_embedding = np.zeros(model.vector_size)\n",
    "    return name_embedding\n",
    "\n",
    "name_embeddings = [get_name_embedding(name, model) for name in all_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clustering on embeddings\n",
    "clustering_model = DBSCAN(eps=0.5, min_samples=1, metric='cosine')\n",
    "cluster_labels = clustering_model.fit_predict(name_embeddings)\n",
    "\n",
    "# Group entities by clusters\n",
    "vector_clusters = {}\n",
    "for label, name in zip(cluster_labels, all_entities):\n",
    "    vector_clusters.setdefault(label, []).append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clusters from both methods\n",
    "# Map entities to their rule-based cluster IDs\n",
    "entity_to_rule_cluster = {}\n",
    "for cluster_id, cluster in enumerate(clusters):\n",
    "    for entity in cluster:\n",
    "        entity_to_rule_cluster[entity] = cluster_id\n",
    "\n",
    "# Merge clusters based on overlapping entities\n",
    "combined_clusters = {}\n",
    "for vector_cluster_id, vector_cluster in vector_clusters.items():\n",
    "    combined_cluster = set()\n",
    "    for entity in vector_cluster:\n",
    "        rule_cluster_id = entity_to_rule_cluster.get(entity)\n",
    "        if rule_cluster_id is not None:\n",
    "            combined_cluster.update(clusters[rule_cluster_id])\n",
    "        else:\n",
    "            combined_cluster.add(entity)\n",
    "    combined_clusters[vector_cluster_id] = list(combined_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_to_representative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply the resolution to the 'Person_Entities' column\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResolved_Person_Entities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPerson_Entities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolve_entities\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mediagraph/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mresolve_entities\u001b[0;34m(entities)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[1;32m      5\u001b[0m     preprocessed_name \u001b[38;5;241m=\u001b[39m preprocess_name(name)\n\u001b[0;32m----> 6\u001b[0m     representative_name \u001b[38;5;241m=\u001b[39m \u001b[43mentity_to_representative\u001b[49m\u001b[38;5;241m.\u001b[39mget(preprocessed_name, name)\n\u001b[1;32m      7\u001b[0m     resolved\u001b[38;5;241m.\u001b[39mappend(representative_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_to_representative' is not defined"
     ]
    }
   ],
   "source": [
    "# Update the DataFrame with resolved entities\n",
    "def resolve_entities(entities):\n",
    "    resolved = []\n",
    "    for name in entities:\n",
    "        preprocessed_name = preprocess_name(name)\n",
    "        representative_name = entity_to_representative.get(preprocessed_name, name)\n",
    "        resolved.append(representative_name)\n",
    "    return resolved\n",
    "\n",
    "# Apply the resolution to the 'Person_Entities' column\n",
    "df['Resolved_Person_Entities'] = df['Person_Entities'].apply(resolve_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import re\n",
    "from metaphone import doublemetaphone\n",
    "from difflib import SequenceMatcher\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/home/vidur/mediagraph/oldPythonFiles/2024output_PreAlias.csv')\n",
    "\n",
    "# Fill missing values in 'Text' column\n",
    "df['Text'] = df['Text'].fillna('')\n",
    "\n",
    "# Parse the 'Resolved Entities' column to extract entities\n",
    "def parse_resolved_entities(resolved_entities_str):\n",
    "    try:\n",
    "        # Convert the string representation of the list to an actual list\n",
    "        entities = literal_eval(resolved_entities_str)\n",
    "        return entities\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the parsing function\n",
    "df['Parsed_Entities'] = df['Resolved Entities'].apply(parse_resolved_entities)\n",
    "\n",
    "# Extract person-type entities\n",
    "def extract_person_entities(entities):\n",
    "    return [entity[0] for entity in entities if entity[1] in ['PERSON', 'POL', 'DIR']]\n",
    "\n",
    "# Apply extraction to get person entities\n",
    "df['Person_Entities'] = df['Parsed_Entities'].apply(extract_person_entities)\n",
    "\n",
    "# Collect all person entities from all articles\n",
    "all_entities = df['Person_Entities'].explode().dropna().unique().tolist()\n",
    "\n",
    "# Preprocess entity names\n",
    "def preprocess_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z\\s\\']', '', name)\n",
    "    name = name.lower().strip()\n",
    "    return name\n",
    "\n",
    "all_entities = [preprocess_name(name) for name in all_entities]\n",
    "\n",
    "# Rule-Based Matching Functions\n",
    "def levenshtein_ratio(s1, s2):\n",
    "    return SequenceMatcher(None, s1, s2).ratio()\n",
    "\n",
    "def metaphone_match(name1, name2):\n",
    "    meta1 = doublemetaphone(name1)\n",
    "    meta2 = doublemetaphone(name2)\n",
    "    return any(m1 == m2 for m1 in meta1 for m2 in meta2 if m1 and m2)\n",
    "\n",
    "def check_match(name1, name2):\n",
    "    # Remove periods, convert to lowercase, and split names\n",
    "    name1_words = name1.replace('.', '').lower().split()\n",
    "    name2_words = name2.replace('.', '').lower().split()\n",
    "    \n",
    "    # Check if either name is a single word (ignore such cases)\n",
    "    if len(name1_words) < 2 or len(name2_words) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Extract last names\n",
    "    last_name1 = name1_words[-1]\n",
    "    last_name2 = name2_words[-1]\n",
    "    \n",
    "    # Check if last names match or if one is the initial of the other\n",
    "    last_names_match = (last_name1 == last_name2 or \n",
    "                        (len(last_name1) == 1 and last_name1 == last_name2[0]) or \n",
    "                        (len(last_name2) == 1 and last_name2 == last_name1[0]))\n",
    "    \n",
    "    # Extract first names or initials\n",
    "    first_name1 = name1_words[0]\n",
    "    first_name2 = name2_words[0]\n",
    "    \n",
    "    # Check if first names match or initials correspond\n",
    "    first_names_match = (first_name1[0] == first_name2[0])\n",
    "    \n",
    "    # Use a lower threshold for Levenshtein ratio when comparing full names\n",
    "    if len(first_name1) > 1 and len(first_name2) > 1:\n",
    "        lev_ratio = levenshtein_ratio(first_name1, first_name2)\n",
    "    else:\n",
    "        lev_ratio = 1  # Assume perfect match if one is just an initial\n",
    "    \n",
    "    # Check Metaphone phonetic similarity for both first and last names combined\n",
    "    meta_match = metaphone_match(' '.join(name1_words), ' '.join(name2_words))\n",
    "    \n",
    "    # Determine final match based on OR condition\n",
    "    if (last_names_match and (first_names_match or lev_ratio >= 0.8)) or meta_match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Build clusters based on rule-based matching\n",
    "clusters = []\n",
    "unclustered_entities = set(all_entities)\n",
    "\n",
    "while unclustered_entities:\n",
    "    base_entity = unclustered_entities.pop()\n",
    "    cluster = [base_entity]\n",
    "    to_check = set(unclustered_entities)\n",
    "    for other_entity in to_check:\n",
    "        if check_match(base_entity, other_entity):\n",
    "            cluster.append(other_entity)\n",
    "            unclustered_entities.remove(other_entity)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "# Build a corpus for Word2Vec (using the text data)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['Tokens'] = df['Text'].apply(preprocess_text)\n",
    "corpus = df['Tokens'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "embedding_size = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "model = Word2Vec(sentences=corpus,\n",
    "                 vector_size=embedding_size,\n",
    "                 window=window_size,\n",
    "                 min_count=min_count,\n",
    "                 workers=workers,\n",
    "                 sg=0)\n",
    "\n",
    "# Generate embeddings for entities\n",
    "def get_name_embedding(name, model):\n",
    "    words = name.split()\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "        else:\n",
    "            word_vectors.append(np.zeros(model.vector_size))\n",
    "    if word_vectors:\n",
    "        name_embedding = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        name_embedding = np.zeros(model.vector_size)\n",
    "    return name_embedding\n",
    "\n",
    "name_embeddings = [get_name_embedding(name, model) for name in all_entities]\n",
    "\n",
    "# Apply clustering on embeddings\n",
    "clustering_model = DBSCAN(eps=0.5, min_samples=1, metric='cosine')\n",
    "cluster_labels = clustering_model.fit_predict(name_embeddings)\n",
    "\n",
    "# Group entities by clusters\n",
    "vector_clusters = {}\n",
    "for label, name in zip(cluster_labels, all_entities):\n",
    "    vector_clusters.setdefault(label, []).append(name)\n",
    "\n",
    "# Combine clusters from both methods\n",
    "# Map entities to their rule-based cluster IDs\n",
    "entity_to_rule_cluster = {}\n",
    "for cluster_id, cluster in enumerate(clusters):\n",
    "    for entity in cluster:\n",
    "        entity_to_rule_cluster[entity] = cluster_id\n",
    "\n",
    "# Merge clusters based on overlapping entities\n",
    "combined_clusters = {}\n",
    "for vector_cluster_id, vector_cluster in vector_clusters.items():\n",
    "    combined_cluster = set()\n",
    "    for entity in vector_cluster:\n",
    "        rule_cluster_id = entity_to_rule_cluster.get(entity)\n",
    "        if rule_cluster_id is not None:\n",
    "            combined_cluster.update(clusters[rule_cluster_id])\n",
    "        else:\n",
    "            combined_cluster.add(entity)\n",
    "    combined_clusters[vector_cluster_id] = list(combined_cluster)\n",
    "\n",
    "# Create a mapping from entity to representative name\n",
    "entity_to_representative = {}\n",
    "for cluster_id, name_list in combined_clusters.items():\n",
    "    # Decide on a representative name for the entity (e.g., the longest name)\n",
    "    representative_name = max(name_list, key=len)\n",
    "    for name in name_list:\n",
    "        entity_to_representative[name] = representative_name\n",
    "\n",
    "# Update the DataFrame with resolved entities\n",
    "def resolve_entities(entities, entity_to_representative):\n",
    "    resolved = []\n",
    "    for name in entities:\n",
    "        preprocessed_name = preprocess_name(name)\n",
    "        representative_name = entity_to_representative.get(preprocessed_name, name)\n",
    "        resolved.append(representative_name)\n",
    "    return resolved\n",
    "\n",
    "# Apply the resolution to the 'Person_Entities' column\n",
    "df['Resolved_Person_Entities'] = df['Person_Entities'].apply(lambda x: resolve_entities(x, entity_to_representative))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modi', 'Shivraj Singh Chouhan', 'Savitri Thakur', 'Bharti Pardhi', 'Cong']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Person_Entities'][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'output_final2.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
